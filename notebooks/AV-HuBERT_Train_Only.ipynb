{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# AV-HuBERT \u2014 Train Only (CTC / Seq2Seq)\n",
        "\n",
        "This notebook is **cleaned for training only**. All data prep happens offline; upload the prepared directory that contains `train.tsv`, `valid.tsv`, `dict.wrd.txt` (and optional SPM files if using Seq2Seq).\n",
        "\n",
        "### What\u2019s inside\n",
        "1. Runtime & repo setup\n",
        "2. User config (paths; choose CTC or Seq2Seq)\n",
        "3. Sanity checks on your uploaded manifests\n",
        "4. **CTC fine-tuning** (low-VRAM defaults; resume-safe)\n",
        "5. **Resume** (just re-run the same cell)\n",
        "6. Optional **Seq2Seq** launch stub (if you later want punctuation/casing)\n",
        "\n",
        "> Tip: Resuming is as simple as running the same training cell again with the same `RUN_DIR` \u2014 Fairseq loads `checkpoint_last.pt` automatically.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "setup"
      },
      "source": [
        "## 1) Runtime & repo setup\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "!nvidia-smi || true\n",
        "\n",
        "# If you already have the repo somewhere, set REPO_DIR in the next cell.\n",
        "# Otherwise, clone a fresh copy here (uncomment the next two lines):\n",
        "# !git clone https://github.com/facebookresearch/av_hubert.git /content/av_hubert\n",
        "# !echo \"Cloned AV-HuBERT into /content/av_hubert\"\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "config"
      },
      "source": [
        "## 2) User config \u2014 edit these\n",
        "REPO_DIR = '/content/av_hubert'  # repo root (contains 'avhubert/' and 'fairseq/' folders)\n",
        "DATA_DIR = '/content/drive/MyDrive/tcdtimit/volunteers/01M/Clips/433h_data'  # your prepared manifests dir\n",
        "CKPT     = '/content/drive/MyDrive/AVSR_Colab/models/base_vox_iter5.pt'     # pre-trained checkpoint\n",
        "RUN_DIR  = '/content/drive/MyDrive/tcdtimit/volunteers/01M/runs/ctc_clean'   # experiment folder (resume-safe)\n",
        "\n",
        "# Training flavor: 'ctc' (lighter/faster) or 's2s' (Seq2Seq; needs SentencePiece model)\n",
        "FINETUNE_STYLE = 'ctc'  # 'ctc' or 's2s'\n",
        "\n",
        "# Modalities: '[\"audio\",\"video\"]' or '[\"audio\"]' if lips are poor\n",
        "MODALITIES = '[\"audio\",\"video\"]'\n",
        "\n",
        "# VRAM-friendly defaults\n",
        "STACK_ORDER_AUDIO = 4     # (16kHz + 25 fps labels) => 26*4=104-dim features\n",
        "MAX_TOKENS        = 100000 # reduce if you still OOM (e.g., 60000)\n",
        "UPDATE_FREQ       = 2      # gradient accumulation\n",
        "NUM_WORKERS       = 1\n",
        "\n",
        "# Env\n",
        "import os, pathlib\n",
        "os.environ['HYDRA_FULL_ERROR'] = '1'\n",
        "os.environ['PYTORCH_CUDA_ALLOC_CONF'] = 'max_split_size_mb:128'\n",
        "os.environ['PYTHONPATH'] = f\"{REPO_DIR}/avhubert:\" + os.environ.get('PYTHONPATH','')\n",
        "pathlib.Path(RUN_DIR).mkdir(parents=True, exist_ok=True)\n",
        "print('Using:\\n  REPO_DIR=', REPO_DIR, '\\n  DATA_DIR=', DATA_DIR, '\\n  CKPT=', CKPT, '\\n  RUN_DIR=', RUN_DIR)\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "deps"
      },
      "source": [
        "## 3) Install Python deps & make sure fairseq is importable\n",
        "%pip -q install tensorboard hydra-core==1.1.* omegaconf==2.1.* sentencepiece opencv-python-headless==4.8.* >/dev/null\n",
        "%cd $REPO_DIR/fairseq\n",
        "%pip -q install -e . >/dev/null\n",
        "%cd $REPO_DIR\n",
        "import fairseq; import avhubert\n",
        "print('fairseq version OK; avhubert import OK')\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sanity"
      },
      "source": [
        "## 4) Sanity checks on manifests\n",
        "import os\n",
        "req = [f\"{DATA_DIR}/train.tsv\", f\"{DATA_DIR}/valid.tsv\", f\"{DATA_DIR}/dict.wrd.txt\"]\n",
        "missing = [p for p in req if not os.path.isfile(p)]\n",
        "assert not missing, f\"Missing files: {missing}\"\n",
        "print('Found train/valid/dict \u2705')\n",
        "print('train.tsv head:')\n",
        "with open(f\"{DATA_DIR}/train.tsv\") as f:\n",
        "    for i, ln in zip(range(6), f):\n",
        "        print(ln.rstrip())\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "train_ctc"
      },
      "source": [
        "## 5) TRAIN \u2014 CTC (default). Re-run this cell to resume (same RUN_DIR)\n",
        "assert FINETUNE_STYLE == 'ctc', 'Switch FINETUNE_STYLE to ctc or use the Seq2Seq cell below.'\n",
        "%%bash -s \"$REPO_DIR\" \"$DATA_DIR\" \"$CKPT\" \"$RUN_DIR\" \"$MODALITIES\" \"$STACK_ORDER_AUDIO\" \"$MAX_TOKENS\" \"$UPDATE_FREQ\" \"$NUM_WORKERS\"\n",
        "set -euo pipefail\n",
        "REPO_DIR=\"$1\"; DATA_DIR=\"$2\"; CKPT=\"$3\"; RUN_DIR=\"$4\"; MODALITIES=\"$5\"; SOA=\"$6\"; MAXTOK=\"$7\"; UPF=\"$8\"; NW=\"$9\"\n",
        "export HYDRA_FULL_ERROR=1\n",
        "export PYTORCH_CUDA_ALLOC_CONF=max_split_size_mb:128\n",
        "export PYTHONPATH=\"${REPO_DIR}/avhubert:${PYTHONPATH:-}\"\n",
        "\n",
        "python -m fairseq_cli.hydra_train \\\n",
        "  --config-dir \"${REPO_DIR}/avhubert/conf/finetune\" \\\n",
        "  --config-name base_vox_30h \\\n",
        "  common.user_dir=\"${REPO_DIR}/avhubert\" \\\n",
        "  hydra.run.dir=\"${RUN_DIR}\" \\\n",
        "  task.data=\"${DATA_DIR}\" \\\n",
        "  task.label_dir=\"${DATA_DIR}\" \\\n",
        "  task.labels='[\"ltr\"]' \\\n",
        "  task.modalities=\"${MODALITIES}\" \\\n",
        "  task.is_s2s=false \\\n",
        "  task.normalize=true \\\n",
        "  task.image_aug=false \\\n",
        "  task.stack_order_audio=\"${SOA}\" \\\n",
        "  task.max_sample_size=null \\\n",
        "  dataset.train_subset=train \\\n",
        "  dataset.valid_subset=valid \\\n",
        "  dataset.num_workers=\"${NW}\" \\\n",
        "  dataset.max_tokens=\"${MAXTOK}\" \\\n",
        "  model._name=av_hubert_ctc \\\n",
        "  model.w2v_path=\"${CKPT}\" \\\n",
        "  model.freeze_finetune_updates=0 \\\n",
        "  model.decoder_layers=3 \\\n",
        "  +model.w2v_args.task._name=av_hubert_pretraining \\\n",
        "  +model.w2v_args.task.data=\"${DATA_DIR}\" \\\n",
        "  +model.w2v_args.task.label_dir=\"${DATA_DIR}\" \\\n",
        "  +model.w2v_args.task.labels='[\"ltr\"]' \\\n",
        "  +model.w2v_args.task.modalities=\"${MODALITIES}\" \\\n",
        "  +model.w2v_args.task.sample_rate=16000 \\\n",
        "  +model.w2v_args.task.label_rate=25 \\\n",
        "  +model.w2v_args.task.pad_audio=true \\\n",
        "  +model.w2v_args.task.random_crop=false \\\n",
        "  +model.w2v_args.task.single_target=true \\\n",
        "  +model.w2v_args.task.fine_tuning=false \\\n",
        "  +model.w2v_args.task.normalize=true \\\n",
        "  +model.w2v_args.task.image_aug=false \\\n",
        "  +model.w2v_args.task.stack_order_audio=\"${SOA}\" \\\n",
        "  +model.w2v_args.label_rate=25 \\\n",
        "  +model.w2v_args.model._name=av_hubert \\\n",
        "  +model.w2v_args.model.input_modality=audio \\\n",
        "  +model.w2v_args.model.audio_feat_dim=$((26*SOA)) \\\n",
        "  +model.w2v_args.model.label_rate=25 \\\n",
        "  criterion._name=ctc \\\n",
        "  +criterion.zero_infinity=true \\\n",
        "  +criterion.post_process=letter \\\n",
        "  optimization.max_update=150 \\\n",
        "  optimization.lr='[1e-4]' \\\n",
        "  optimization.update_freq=\"[${UPF}]\" \\\n",
        "  optimization.clip_norm=5.0 \\\n",
        "  lr_scheduler.warmup_steps=5 \\\n",
        "  lr_scheduler.hold_steps=5 \\\n",
        "  lr_scheduler.decay_steps=140 \\\n",
        "  checkpoint.save_dir=\"${RUN_DIR}/checkpoints\" \\\n",
        "  checkpoint.save_interval_updates=50 \\\n",
        "  checkpoint.best_checkpoint_metric=loss \\\n",
        "  distributed_training.distributed_world_size=1 \\\n",
        "  common.log_interval=5\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 6) Resume training\n",
        "Just re-run the **CTC TRAIN** cell with the same `RUN_DIR`. Fairseq will automatically load `checkpoint_last.pt` if present.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "seq2seq"
      },
      "source": [
        "## 7) OPTIONAL \u2014 Seq2Seq variant (needs SentencePiece model)\n",
        "# Steps before enabling:\n",
        "#   \u2022 Ensure your data dir contains SPM files, e.g. 'spm1000/spm_unigramXXX.model'.\n",
        "#   \u2022 Set TOKENIZER_MODEL below.\n",
        "# Then set FINETUNE_STYLE='s2s' in the config cell and run this cell instead of the CTC one.\n",
        "assert FINETUNE_STYLE == 's2s', 'Switch FINETUNE_STYLE to \"s2s\" to run this cell.'\n",
        "TOKENIZER_MODEL = f\"{DATA_DIR}/spm1000/spm_unigram1000.model\"  # edit if different\n",
        "%%bash -s \"$REPO_DIR\" \"$DATA_DIR\" \"$CKPT\" \"$RUN_DIR\" \"$MODALITIES\" \"$STACK_ORDER_AUDIO\" \"$MAX_TOKENS\" \"$UPDATE_FREQ\" \"$NUM_WORKERS\" \"$TOKENIZER_MODEL\"\n",
        "set -euo pipefail\n",
        "REPO_DIR=\"$1\"; DATA_DIR=\"$2\"; CKPT=\"$3\"; RUN_DIR=\"$4\"; MODALITIES=\"$5\"; SOA=\"$6\"; MAXTOK=\"$7\"; UPF=\"$8\"; NW=\"$9\"; TOK=\"$10\"\n",
        "export HYDRA_FULL_ERROR=1\n",
        "export PYTORCH_CUDA_ALLOC_CONF=max_split_size_mb:128\n",
        "export PYTHONPATH=\"${REPO_DIR}/avhubert:${PYTHONPATH:-}\"\n",
        "\n",
        "python -m fairseq_cli.hydra_train \\\n",
        "  --config-dir \"${REPO_DIR}/avhubert/conf/finetune\" \\\n",
        "  --config-name base_vox_30h \\\n",
        "  common.user_dir=\"${REPO_DIR}/avhubert\" \\\n",
        "  hydra.run.dir=\"${RUN_DIR}\" \\\n",
        "  task.data=\"${DATA_DIR}\" \\\n",
        "  task.label_dir=\"${DATA_DIR}\" \\\n",
        "  task.labels='[\"ltr\"]' \\\n",
        "  task.modalities=\"${MODALITIES}\" \\\n",
        "  task.is_s2s=true \\\n",
        "  task.normalize=true \\\n",
        "  task.image_aug=false \\\n",
        "  task.stack_order_audio=\"${SOA}\" \\\n",
        "  task.max_sample_size=null \\\n",
        "  task.tokenizer_bpe_name=sentencepiece \\\n",
        "  task.tokenizer_bpe_model=\"${TOK}\" \\\n",
        "  dataset.train_subset=train \\\n",
        "  dataset.valid_subset=valid \\\n",
        "  dataset.num_workers=\"${NW}\" \\\n",
        "  dataset.max_tokens=\"${MAXTOK}\" \\\n",
        "  model._name=av_hubert_seq2seq \\\n",
        "  model.w2v_path=\"${CKPT}\" \\\n",
        "  model.freeze_finetune_updates=0 \\\n",
        "  model.decoder_layers=6 \\\n",
        "  +model.w2v_args.task._name=av_hubert_pretraining \\\n",
        "  +model.w2v_args.task.data=\"${DATA_DIR}\" \\\n",
        "  +model.w2v_args.task.label_dir=\"${DATA_DIR}\" \\\n",
        "  +model.w2v_args.task.labels='[\"ltr\"]' \\\n",
        "  +model.w2v_args.task.modalities=\"${MODALITIES}\" \\\n",
        "  +model.w2v_args.task.sample_rate=16000 \\\n",
        "  +model.w2v_args.task.label_rate=25 \\\n",
        "  +model.w2v_args.task.pad_audio=true \\\n",
        "  +model.w2v_args.task.random_crop=false \\\n",
        "  +model.w2v_args.task.single_target=true \\\n",
        "  +model.w2v_args.task.fine_tuning=false \\\n",
        "  +model.w2v_args.task.normalize=true \\\n",
        "  +model.w2v_args.task.image_aug=false \\\n",
        "  +model.w2v_args.task.stack_order_audio=\"${SOA}\" \\\n",
        "  +model.w2v_args.label_rate=25 \\\n",
        "  +model.w2v_args.model._name=av_hubert \\\n",
        "  +model.w2v_args.model.input_modality=audio \\\n",
        "  +model.w2v_args.model.audio_feat_dim=$((26*SOA)) \\\n",
        "  +model.w2v_args.model.label_rate=25 \\\n",
        "  criterion._name=label_smoothed_cross_entropy \\\n",
        "  optimization.max_update=150 \\\n",
        "  optimization.lr='[5e-5]' \\\n",
        "  optimization.update_freq=\"[${UPF}]\" \\\n",
        "  optimization.clip_norm=5.0 \\\n",
        "  lr_scheduler.warmup_steps=5 \\\n",
        "  lr_scheduler.hold_steps=5 \\\n",
        "  lr_scheduler.decay_steps=140 \\\n",
        "  checkpoint.save_dir=\"${RUN_DIR}/checkpoints\" \\\n",
        "  checkpoint.save_interval_updates=50 \\\n",
        "  checkpoint.best_checkpoint_metric=loss \\\n",
        "  distributed_training.distributed_world_size=1 \\\n",
        "  common.log_interval=5\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Notes\n",
        "- **Prepared data expectations:**\n",
        "  Your `$DATA_DIR` should look like:\n",
        "  ```\n",
        "  train.tsv\n",
        "  valid.tsv\n",
        "  dict.wrd.txt\n",
        "  audio/  (WAVs at 16kHz, filenames like <id>.wav)\n",
        "  video/  (MP4s at 25fps, filenames like <id>.mp4)\n",
        "  spm1000/ (optional for Seq2Seq)\n",
        "    \u251c\u2500 spm_unigramXXXX.model\n",
        "    \u2514\u2500 spm_unigramXXXX.vocab\n",
        "  ```\n",
        "- **Cancel & resume:** Stopping the runtime is safe. Re-run the CTC train cell \u2014 it will pick up from `checkpoint_last.pt` under `RUN_DIR/checkpoints`.\n",
        "- **Low VRAM tips:** Try `MAX_TOKENS=60000`, `UPDATE_FREQ=3`, and/or `task.image_aug=false`.\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.10"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}