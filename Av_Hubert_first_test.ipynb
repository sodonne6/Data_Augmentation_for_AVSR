{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyNKF54MXesAY9xQu4ZgDdRM",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/sodonne6/Data_Augmentation_for_AVSR/blob/main/Av_Hubert_first_test.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%pip -q install -U condacolab\n",
        "import condacolab\n",
        "condacolab.install() #restarts runtime"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "d6ljFoX6Z5cf",
        "outputId": "a3ebee72-180f-451a-cc41-45489b7c987c"
      },
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "âœ¨ðŸ°âœ¨ Everything looks OK!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#create and activate python 3.8 env\n",
        "%%bash\n",
        "eval \"$(conda shell.bash hook)\"\n",
        "conda env remove -n av_hubert || true # Remove the environment if it exists\n",
        "conda create -n av_hubert python=3.8\n",
        "conda activate av_hubert\n",
        "\n",
        "# clone and install dependencies inside env\n",
        "git clone https://github.com/facebookresearch/av_hubert.git /content/av_hubert\n",
        "cd /content/av_hubert\n",
        "git submodule init\n",
        "git submodule update\n",
        "\n",
        "#show content for sanity check\n",
        "pwd\n",
        "ls -lah"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "J8wsPehEaSAU",
        "outputId": "7a8d5980-e0a5-494b-9efe-9bf00a771fbf"
      },
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Remove all packages in environment /usr/local/envs/av_hubert:\n",
            "\n",
            "\n",
            "## Package Plan ##\n",
            "\n",
            "  environment location: /usr/local/envs/av_hubert\n",
            "\n",
            "\n",
            "The following packages will be REMOVED:\n",
            "\n",
            "  _libgcc_mutex-0.1-conda_forge\n",
            "  _openmp_mutex-4.5-2_gnu\n",
            "  bzip2-1.0.8-hda65f42_8\n",
            "  ca-certificates-2025.8.3-hbd8a1cb_0\n",
            "  ld_impl_linux-64-2.44-h1423503_1\n",
            "  libffi-3.4.6-h2dba641_1\n",
            "  libgcc-15.1.0-h767d61c_5\n",
            "  libgcc-ng-15.1.0-h69a702a_5\n",
            "  libgomp-15.1.0-h767d61c_5\n",
            "  liblzma-5.8.1-hb9d3cd8_2\n",
            "  liblzma-devel-5.8.1-hb9d3cd8_2\n",
            "  libnsl-2.0.1-hb9d3cd8_1\n",
            "  libsqlite-3.50.4-h0c1763c_0\n",
            "  libuuid-2.41.2-he9a06e4_0\n",
            "  libxcrypt-4.4.36-hd590300_1\n",
            "  libzlib-1.3.1-hb9d3cd8_2\n",
            "  ncurses-6.5-h2d0b736_3\n",
            "  openssl-3.5.3-h26f9b46_1\n",
            "  pip-24.3.1-pyh8b19718_0\n",
            "  python-3.8.20-h4a871b0_2_cpython\n",
            "  readline-8.2-h8c095d6_2\n",
            "  setuptools-75.3.0-pyhd8ed1ab_0\n",
            "  tk-8.6.13-noxft_hd72426e_102\n",
            "  wheel-0.45.1-pyhd8ed1ab_0\n",
            "  xz-5.8.1-hbcc6ac9_2\n",
            "  xz-gpl-tools-5.8.1-hbcc6ac9_2\n",
            "  xz-tools-5.8.1-hb9d3cd8_2\n",
            "\n",
            "\n",
            "\n",
            "Downloading and Extracting Packages: ...working... done\n",
            "Preparing transaction: - \b\bdone\n",
            "Verifying transaction: | \b\b/ \b\bdone\n",
            "Executing transaction: \\ \b\b| \b\b/ \b\b- \b\bdone\n",
            "Channels:\n",
            " - conda-forge\n",
            "Platform: linux-64\n",
            "Collecting package metadata (repodata.json): ...working... done\n",
            "Solving environment: ...working... done\n",
            "\n",
            "## Package Plan ##\n",
            "\n",
            "  environment location: /usr/local/envs/av_hubert\n",
            "\n",
            "  added / updated specs:\n",
            "    - python=3.8\n",
            "\n",
            "\n",
            "The following NEW packages will be INSTALLED:\n",
            "\n",
            "  _libgcc_mutex      conda-forge/linux-64::_libgcc_mutex-0.1-conda_forge \n",
            "  _openmp_mutex      conda-forge/linux-64::_openmp_mutex-4.5-2_gnu \n",
            "  bzip2              conda-forge/linux-64::bzip2-1.0.8-hda65f42_8 \n",
            "  ca-certificates    conda-forge/noarch::ca-certificates-2025.8.3-hbd8a1cb_0 \n",
            "  ld_impl_linux-64   conda-forge/linux-64::ld_impl_linux-64-2.44-h1423503_1 \n",
            "  libffi             conda-forge/linux-64::libffi-3.4.6-h2dba641_1 \n",
            "  libgcc             conda-forge/linux-64::libgcc-15.1.0-h767d61c_5 \n",
            "  libgcc-ng          conda-forge/linux-64::libgcc-ng-15.1.0-h69a702a_5 \n",
            "  libgomp            conda-forge/linux-64::libgomp-15.1.0-h767d61c_5 \n",
            "  liblzma            conda-forge/linux-64::liblzma-5.8.1-hb9d3cd8_2 \n",
            "  liblzma-devel      conda-forge/linux-64::liblzma-devel-5.8.1-hb9d3cd8_2 \n",
            "  libnsl             conda-forge/linux-64::libnsl-2.0.1-hb9d3cd8_1 \n",
            "  libsqlite          conda-forge/linux-64::libsqlite-3.50.4-h0c1763c_0 \n",
            "  libuuid            conda-forge/linux-64::libuuid-2.41.2-he9a06e4_0 \n",
            "  libxcrypt          conda-forge/linux-64::libxcrypt-4.4.36-hd590300_1 \n",
            "  libzlib            conda-forge/linux-64::libzlib-1.3.1-hb9d3cd8_2 \n",
            "  ncurses            conda-forge/linux-64::ncurses-6.5-h2d0b736_3 \n",
            "  openssl            conda-forge/linux-64::openssl-3.5.3-h26f9b46_1 \n",
            "  pip                conda-forge/noarch::pip-24.3.1-pyh8b19718_0 \n",
            "  python             conda-forge/linux-64::python-3.8.20-h4a871b0_2_cpython \n",
            "  readline           conda-forge/linux-64::readline-8.2-h8c095d6_2 \n",
            "  setuptools         conda-forge/noarch::setuptools-75.3.0-pyhd8ed1ab_0 \n",
            "  tk                 conda-forge/linux-64::tk-8.6.13-noxft_hd72426e_102 \n",
            "  wheel              conda-forge/noarch::wheel-0.45.1-pyhd8ed1ab_0 \n",
            "  xz                 conda-forge/linux-64::xz-5.8.1-hbcc6ac9_2 \n",
            "  xz-gpl-tools       conda-forge/linux-64::xz-gpl-tools-5.8.1-hbcc6ac9_2 \n",
            "  xz-tools           conda-forge/linux-64::xz-tools-5.8.1-hb9d3cd8_2 \n",
            "\n",
            "\n",
            "\n",
            "Downloading and Extracting Packages: ...working... done\n",
            "Preparing transaction: - \b\b\\ \b\b| \b\b/ \b\bdone\n",
            "Verifying transaction: \\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\bdone\n",
            "Executing transaction: | \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\bdone\n",
            "#\n",
            "# To activate this environment, use\n",
            "#\n",
            "#     $ conda activate av_hubert\n",
            "#\n",
            "# To deactivate an active environment, use\n",
            "#\n",
            "#     $ conda deactivate\n",
            "\n",
            "/content/av_hubert\n",
            "total 60K\n",
            "drwxr-xr-x 6 root root 4.0K Sep 23 14:58 .\n",
            "drwxr-xr-x 1 root root 4.0K Sep 23 17:29 ..\n",
            "drwxr-xr-x 2 root root 4.0K Sep 23 14:58 assets\n",
            "drwxr-xr-x 7 root root 4.0K Sep 23 15:12 avhubert\n",
            "-rw-r--r-- 1 root root 3.5K Sep 23 14:58 CODE_OF_CONDUCT.md\n",
            "-rw-r--r-- 1 root root 1.3K Sep 23 14:58 CONTRIBUTING.md\n",
            "drwxr-xr-x 9 root root 4.0K Sep 23 14:58 fairseq\n",
            "drwxr-xr-x 9 root root 4.0K Sep 23 14:58 .git\n",
            "-rw-r--r-- 1 root root   80 Sep 23 14:58 .gitmodules\n",
            "-rw-r--r-- 1 root root 8.7K Sep 23 14:58 LICENSE\n",
            "-rw-r--r-- 1 root root 6.5K Sep 23 14:58 README.md\n",
            "-rw-r--r-- 1 root root  108 Sep 23 14:58 requirements.txt\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n",
            "\n",
            "==> WARNING: A newer version of conda exists. <==\n",
            "    current version: 24.11.2\n",
            "    latest version: 25.7.0\n",
            "\n",
            "Please update conda by running\n",
            "\n",
            "    $ conda update -n base -c conda-forge conda\n",
            "\n",
            "\n",
            "fatal: destination path '/content/av_hubert' already exists and is not an empty directory.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# install deps inside the conda env\n",
        "%%bash\n",
        "eval \"$(conda shell.bash hook)\"\n",
        "conda activate av_hubert\n",
        "cd /content/av_hubert/\n",
        "\n",
        "# 0) Repo requirements\n",
        "pip install -r requirements.txt\n",
        "\n",
        "# 1) Pin NumPy so np.float exists\n",
        "pip install --no-cache-dir \"numpy==1.23.5\"\n",
        "\n",
        "# 2) Pin config stack that works with this fairseq tree\n",
        "pip uninstall -y omegaconf hydra-core || true\n",
        "pip install --no-cache-dir omegaconf==2.1.1 hydra-core==1.1.0\n",
        "\n",
        "# 3) Torch (GPU build via pip cu117 wheels)\n",
        "conda remove -y pytorch torchvision torchaudio pytorch-cuda || true\n",
        "pip uninstall -y torch torchvision torchaudio || true\n",
        "pip install --no-cache-dir \\\n",
        "  torch==1.13.1+cu117 \\\n",
        "  torchvision==0.14.1+cu117 \\\n",
        "  torchaudio==0.13.1 \\\n",
        "  --extra-index-url https://download.pytorch.org/whl/cu117\n",
        "\n",
        "# 4) Install fairseq from the bundled submodule (editable) **without deps**\n",
        "cd fairseq\n",
        "pip install -e . --no-deps\n",
        "cd ..\n",
        "\n",
        "# 5) Re-assert the pins in case anything tried to downgrade them (belt & suspenders)\n",
        "pip install --no-cache-dir --upgrade --upgrade-strategy only-if-needed \\\n",
        "  omegaconf==2.1.1 hydra-core==1.1.0\n",
        "\n",
        "# 6) Tiny import check\n",
        "python - << 'PY'\n",
        "import torch, fairseq, numpy as np\n",
        "from omegaconf import II, MISSING\n",
        "import hydra\n",
        "print(\"Torch:\", torch.__version__, \"| CUDA available:\", torch.cuda.is_available())\n",
        "print(\"Fairseq import OK\")\n",
        "print(\"NumPy:\", np.__version__)\n",
        "print(\"OmegaConf II/MISSING present:\", II is not None and MISSING is not None)\n",
        "print(\"Hydra:\", hydra.__version__)\n",
        "PY\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jkNCDgBHcQgr",
        "outputId": "b3d6d827-a4b5-498d-8874-699cefc78766"
      },
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting python-speech-features==0.6 (from -r requirements.txt (line 1))\n",
            "  Using cached python_speech_features-0.6-py3-none-any.whl\n",
            "Collecting scipy==1.10.0 (from -r requirements.txt (line 2))\n",
            "  Using cached scipy-1.10.0-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (58 kB)\n",
            "Collecting opencv-python==4.5.4.60 (from -r requirements.txt (line 3))\n",
            "  Using cached opencv_python-4.5.4.60-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (18 kB)\n",
            "Collecting sentencepiece==0.1.96 (from -r requirements.txt (line 4))\n",
            "  Using cached sentencepiece-0.1.96-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (10 kB)\n",
            "Collecting editdistance==0.6.0 (from -r requirements.txt (line 5))\n",
            "  Using cached editdistance-0.6.0-cp38-cp38-manylinux2010_x86_64.whl.metadata (4.7 kB)\n",
            "Collecting numpy<1.27.0,>=1.19.5 (from scipy==1.10.0->-r requirements.txt (line 2))\n",
            "  Using cached numpy-1.24.4-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (5.6 kB)\n",
            "Using cached scipy-1.10.0-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (34.5 MB)\n",
            "Using cached opencv_python-4.5.4.60-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (60.3 MB)\n",
            "Using cached sentencepiece-0.1.96-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.2 MB)\n",
            "Using cached editdistance-0.6.0-cp38-cp38-manylinux2010_x86_64.whl (286 kB)\n",
            "Using cached numpy-1.24.4-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (17.3 MB)\n",
            "Installing collected packages: sentencepiece, python-speech-features, numpy, editdistance, scipy, opencv-python\n",
            "Successfully installed editdistance-0.6.0 numpy-1.24.4 opencv-python-4.5.4.60 python-speech-features-0.6 scipy-1.10.0 sentencepiece-0.1.96\n",
            "Collecting numpy==1.23.5\n",
            "  Downloading numpy-1.23.5-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (2.3 kB)\n",
            "Downloading numpy-1.23.5-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (17.1 MB)\n",
            "   â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â” 17.1/17.1 MB 133.3 MB/s eta 0:00:00\n",
            "Installing collected packages: numpy\n",
            "  Attempting uninstall: numpy\n",
            "    Found existing installation: numpy 1.24.4\n",
            "    Uninstalling numpy-1.24.4:\n",
            "      Successfully uninstalled numpy-1.24.4\n",
            "Successfully installed numpy-1.23.5\n",
            "Collecting omegaconf==2.1.1\n",
            "  Downloading omegaconf-2.1.1-py3-none-any.whl.metadata (3.6 kB)\n",
            "Collecting hydra-core==1.1.0\n",
            "  Downloading hydra_core-1.1.0-py3-none-any.whl.metadata (4.1 kB)\n",
            "Collecting antlr4-python3-runtime==4.8 (from omegaconf==2.1.1)\n",
            "  Downloading antlr4-python3-runtime-4.8.tar.gz (112 kB)\n",
            "  Preparing metadata (setup.py): started\n",
            "  Preparing metadata (setup.py): finished with status 'done'\n",
            "Collecting PyYAML>=5.1.0 (from omegaconf==2.1.1)\n",
            "  Downloading PyYAML-6.0.2-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (2.1 kB)\n",
            "Collecting importlib-resources (from hydra-core==1.1.0)\n",
            "  Downloading importlib_resources-6.4.5-py3-none-any.whl.metadata (4.0 kB)\n",
            "Collecting zipp>=3.1.0 (from importlib-resources->hydra-core==1.1.0)\n",
            "  Downloading zipp-3.20.2-py3-none-any.whl.metadata (3.7 kB)\n",
            "Downloading omegaconf-2.1.1-py3-none-any.whl (74 kB)\n",
            "Downloading hydra_core-1.1.0-py3-none-any.whl (144 kB)\n",
            "Downloading PyYAML-6.0.2-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (746 kB)\n",
            "   â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â” 746.5/746.5 kB 43.7 MB/s eta 0:00:00\n",
            "Downloading importlib_resources-6.4.5-py3-none-any.whl (36 kB)\n",
            "Downloading zipp-3.20.2-py3-none-any.whl (9.2 kB)\n",
            "Building wheels for collected packages: antlr4-python3-runtime\n",
            "  Building wheel for antlr4-python3-runtime (setup.py): started\n",
            "  Building wheel for antlr4-python3-runtime (setup.py): finished with status 'done'\n",
            "  Created wheel for antlr4-python3-runtime: filename=antlr4_python3_runtime-4.8-py3-none-any.whl size=141213 sha256=85baf2248bc738a29c18fb8ca8a41bb467f9cd0df424cdff4fddd167176981c7\n",
            "  Stored in directory: /tmp/pip-ephem-wheel-cache-f9ueteo1/wheels/c8/d0/ab/d43c02eaddc5b9004db86950802442ad9a26f279c619e28da0\n",
            "Successfully built antlr4-python3-runtime\n",
            "Installing collected packages: antlr4-python3-runtime, zipp, PyYAML, omegaconf, importlib-resources, hydra-core\n",
            "Successfully installed PyYAML-6.0.2 antlr4-python3-runtime-4.8 hydra-core-1.1.0 importlib-resources-6.4.5 omegaconf-2.1.1 zipp-3.20.2\n",
            "Looking in indexes: https://pypi.org/simple, https://download.pytorch.org/whl/cu117\n",
            "Collecting torch==1.13.1+cu117\n",
            "  Downloading https://download.pytorch.org/whl/cu117/torch-1.13.1%2Bcu117-cp38-cp38-linux_x86_64.whl (1801.8 MB)\n",
            "     â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â” 1.8/1.8 GB 154.0 MB/s eta 0:00:00\n",
            "Collecting torchvision==0.14.1+cu117\n",
            "  Downloading https://download.pytorch.org/whl/cu117/torchvision-0.14.1%2Bcu117-cp38-cp38-linux_x86_64.whl (24.3 MB)\n",
            "     â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â” 24.3/24.3 MB 220.5 MB/s eta 0:00:00\n",
            "Collecting torchaudio==0.13.1\n",
            "  Downloading https://download.pytorch.org/whl/cu117/torchaudio-0.13.1%2Bcu117-cp38-cp38-linux_x86_64.whl (4.2 MB)\n",
            "     â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â” 4.2/4.2 MB 201.3 MB/s eta 0:00:00\n",
            "Collecting typing-extensions (from torch==1.13.1+cu117)\n",
            "  Downloading typing_extensions-4.13.2-py3-none-any.whl.metadata (3.0 kB)\n",
            "Requirement already satisfied: numpy in /usr/local/envs/av_hubert/lib/python3.8/site-packages (from torchvision==0.14.1+cu117) (1.23.5)\n",
            "Collecting requests (from torchvision==0.14.1+cu117)\n",
            "  Downloading requests-2.32.4-py3-none-any.whl.metadata (4.9 kB)\n",
            "Collecting pillow!=8.3.*,>=5.3.0 (from torchvision==0.14.1+cu117)\n",
            "  Downloading pillow-10.4.0-cp38-cp38-manylinux_2_28_x86_64.whl.metadata (9.2 kB)\n",
            "Collecting charset_normalizer<4,>=2 (from requests->torchvision==0.14.1+cu117)\n",
            "  Downloading charset_normalizer-3.4.3-cp38-cp38-manylinux2014_x86_64.manylinux_2_17_x86_64.manylinux_2_28_x86_64.whl.metadata (36 kB)\n",
            "Collecting idna<4,>=2.5 (from requests->torchvision==0.14.1+cu117)\n",
            "  Downloading idna-3.10-py3-none-any.whl.metadata (10 kB)\n",
            "Collecting urllib3<3,>=1.21.1 (from requests->torchvision==0.14.1+cu117)\n",
            "  Downloading urllib3-2.2.3-py3-none-any.whl.metadata (6.5 kB)\n",
            "Collecting certifi>=2017.4.17 (from requests->torchvision==0.14.1+cu117)\n",
            "  Downloading certifi-2025.8.3-py3-none-any.whl.metadata (2.4 kB)\n",
            "Downloading pillow-10.4.0-cp38-cp38-manylinux_2_28_x86_64.whl (4.5 MB)\n",
            "   â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â” 4.5/4.5 MB 95.4 MB/s eta 0:00:00\n",
            "Downloading requests-2.32.4-py3-none-any.whl (64 kB)\n",
            "Downloading typing_extensions-4.13.2-py3-none-any.whl (45 kB)\n",
            "Downloading certifi-2025.8.3-py3-none-any.whl (161 kB)\n",
            "Downloading charset_normalizer-3.4.3-cp38-cp38-manylinux2014_x86_64.manylinux_2_17_x86_64.manylinux_2_28_x86_64.whl (147 kB)\n",
            "Downloading idna-3.10-py3-none-any.whl (70 kB)\n",
            "Downloading urllib3-2.2.3-py3-none-any.whl (126 kB)\n",
            "Installing collected packages: urllib3, typing-extensions, pillow, idna, charset_normalizer, certifi, torch, requests, torchvision, torchaudio\n",
            "Successfully installed certifi-2025.8.3 charset_normalizer-3.4.3 idna-3.10 pillow-10.4.0 requests-2.32.4 torch-1.13.1+cu117 torchaudio-0.13.1+cu117 torchvision-0.14.1+cu117 typing-extensions-4.13.2 urllib3-2.2.3\n",
            "Obtaining file:///content/av_hubert/fairseq\n",
            "  Installing build dependencies: started\n",
            "  Installing build dependencies: finished with status 'done'\n",
            "  Checking if build backend supports build_editable: started\n",
            "  Checking if build backend supports build_editable: finished with status 'done'\n",
            "  Getting requirements to build editable: started\n",
            "  Getting requirements to build editable: finished with status 'done'\n",
            "  Installing backend dependencies: started\n",
            "  Installing backend dependencies: finished with status 'done'\n",
            "  Preparing editable metadata (pyproject.toml): started\n",
            "  Preparing editable metadata (pyproject.toml): finished with status 'done'\n",
            "Building wheels for collected packages: fairseq\n",
            "  Building editable for fairseq (pyproject.toml): started\n",
            "  Building editable for fairseq (pyproject.toml): finished with status 'done'\n",
            "  Created wheel for fairseq: filename=fairseq-1.0.0a0+afc77bd-0.editable-cp38-cp38-linux_x86_64.whl size=8850 sha256=51a7a432274d5299604ddbf93c1c236744311e2ecb81796b037f1f6a52790b56\n",
            "  Stored in directory: /tmp/pip-ephem-wheel-cache-7brthh6n/wheels/c0/bc/12/b44810d3f46a1b839bfe22323dd4609d9b0c8430ca1e0cc32e\n",
            "Successfully built fairseq\n",
            "Installing collected packages: fairseq\n",
            "Successfully installed fairseq-1.0.0a0+afc77bd\n",
            "Requirement already satisfied: omegaconf==2.1.1 in /usr/local/envs/av_hubert/lib/python3.8/site-packages (2.1.1)\n",
            "Requirement already satisfied: hydra-core==1.1.0 in /usr/local/envs/av_hubert/lib/python3.8/site-packages (1.1.0)\n",
            "Requirement already satisfied: antlr4-python3-runtime==4.8 in /usr/local/envs/av_hubert/lib/python3.8/site-packages (from omegaconf==2.1.1) (4.8)\n",
            "Requirement already satisfied: PyYAML>=5.1.0 in /usr/local/envs/av_hubert/lib/python3.8/site-packages (from omegaconf==2.1.1) (6.0.2)\n",
            "Requirement already satisfied: importlib-resources in /usr/local/envs/av_hubert/lib/python3.8/site-packages (from hydra-core==1.1.0) (6.4.5)\n",
            "Requirement already satisfied: zipp>=3.1.0 in /usr/local/envs/av_hubert/lib/python3.8/site-packages (from importlib-resources->hydra-core==1.1.0) (3.20.2)\n",
            "Torch: 1.13.1+cu117 | CUDA available: False\n",
            "Fairseq import OK\n",
            "NumPy: 1.23.5\n",
            "OmegaConf II/MISSING present: True\n",
            "Hydra: 1.1.0\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING: Skipping omegaconf as it is not installed.\n",
            "WARNING: Skipping hydra-core as it is not installed.\n",
            "\n",
            "PackagesNotFoundError: The following packages are missing from the target environment:\n",
            "  - torchaudio\n",
            "  - pytorch\n",
            "  - torchvision\n",
            "  - pytorch-cuda\n",
            "\n",
            "\n",
            "WARNING: Skipping torch as it is not installed.\n",
            "WARNING: Skipping torchvision as it is not installed.\n",
            "WARNING: Skipping torchaudio as it is not installed.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#CODE BEGINNING -> MOUNT GOOGLE DRIVE\n",
        "#NEEDS TO BE DONE EVERYTIME\n",
        "\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "n8qI4CM8rINi",
        "outputId": "a388e503-316e-4021-afb6-ed74acf1beb8"
      },
      "execution_count": 33,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#choose a model to use\n",
        "model1_name = \"base_vox_iter5.pt\"\n",
        "model2_name = \"base_noise_pt_noise_ft_30h.pt\""
      ],
      "metadata": {
        "id": "nARQqZ5qUvQR"
      },
      "execution_count": 34,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Load in the pretrained model\n"
      ],
      "metadata": {
        "id": "GIWZiNKllw79"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%bash\n",
        "eval \"$(conda shell.bash hook)\"\n",
        "conda activate av_hubert\n",
        "cd /content  # stay outside the repo root\n",
        "\n",
        "# Put the avhubert *subfolder* on PYTHONPATH so top-level imports work\n",
        "PYTHONPATH=/content/av_hubert/avhubert:$PYTHONPATH \\\n",
        "python -u - << 'PY'\n",
        "import os, torch, importlib\n",
        "\n",
        "# 1) Import fairseq first\n",
        "import fairseq\n",
        "from fairseq.tasks import TASK_REGISTRY\n",
        "print(\"fairseq OK; registry size before:\", len(TASK_REGISTRY))\n",
        "\n",
        "# 2) Force-import the files that perform registration\n",
        "#    (these use @register_task / @register_model decorators)\n",
        "importlib.import_module(\"hubert_pretraining\")  # registers 'av_hubert_pretraining'\n",
        "importlib.import_module(\"hubert\")              # registers the AV-HuBERT model\n",
        "\n",
        "# 3) Verify registration\n",
        "from fairseq.tasks import TASK_REGISTRY\n",
        "print(\"Contains 'av_hubert_pretraining':\", \"av_hubert_pretraining\" in TASK_REGISTRY)\n",
        "\n",
        "# 4) Load checkpoint\n",
        "from fairseq.checkpoint_utils import load_model_ensemble_and_task\n",
        "ckpt_path = \"/content/drive/MyDrive/AVSR_Colab/models/base_vox_iter5.pt\"  # adjust if needed\n",
        "assert os.path.exists(ckpt_path), f\"Checkpoint not found: {ckpt_path}\"\n",
        "\n",
        "models, cfg, task = load_model_ensemble_and_task([ckpt_path])\n",
        "model = models[0]\n",
        "\n",
        "print(\"âœ… Loaded AV-HuBERT checkpoint\")\n",
        "print(\"Torch:\", torch.__version__, \"| CUDA:\", torch.cuda.is_available())\n",
        "print(\"Model:\", type(model).__name__, \"| Task:\", type(task).__name__)\n",
        "PY\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Hn5fX9-CHM9O",
        "outputId": "53d914d0-e51e-4cd6-c974-c8f20051fc61"
      },
      "execution_count": 35,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "fairseq OK; registry size before: 25\n",
            "Contains 'av_hubert_pretraining': True\n",
            "2025-09-23 17:32:52 | INFO | hubert_pretraining | current directory is /content\n",
            "2025-09-23 17:32:52 | INFO | hubert_pretraining | AVHubertPretrainingTask Config {'_name': 'av_hubert_pretraining', 'data': '/checkpoint/bshi/data/lrs3//video/wav/envox_tsv/', 'labels': ['km'], 'label_dir': '/checkpoint/bshi/data/lrs3//video/hubert/stitch-iters/envox-iter4-l12c2000/', 'label_rate': 25, 'sample_rate': 25, 'normalize': True, 'enable_padding': False, 'max_sample_size': 2000, 'min_sample_size': 5, 'max_trim_sample_size': 400, 'single_target': False, 'random_crop': True, 'pad_audio': False, 'pdb': False, 'stack_order_audio': 4, 'skip_verify': False, 'image_aug': True, 'image_crop_size': 88, 'image_mean': 0.421, 'image_std': 0.165, 'modalities': ['audio', 'video'], 'is_s2s': False, 'tokenizer_bpe_name': None, 'tokenizer_bpe_model': None, 'noise_wav': '/checkpoint/bshi/data/lrs3//audio/noise/tsv/musan-lgall/', 'noise_prob': 0.25, 'noise_snr': '0', 'noise_num': 1, 'fine_tuning': False}\n",
            "2025-09-23 17:32:52 | INFO | hubert | HubertModel Config: {'_name': 'av_hubert', 'label_rate': 25, 'input_modality': '${task.input_modality}', 'extractor_mode': default, 'encoder_layers': 12, 'encoder_embed_dim': 768, 'encoder_ffn_embed_dim': 3072, 'encoder_attention_heads': 12, 'activation_fn': gelu, 'dropout': 0.1, 'attention_dropout': 0.1, 'activation_dropout': 0.0, 'encoder_layerdrop': 0.05, 'dropout_input': 0.1, 'dropout_features': 0.1, 'final_dim': 256, 'untie_final_proj': True, 'layer_norm_first': True, 'conv_feature_layers': '[(512,10,5)] + [(512,3,2)] * 4 + [(512,2,2)] * 2', 'conv_bias': False, 'logit_temp': 0.1, 'target_glu': False, 'feature_grad_mult': 0.1, 'mask_length_audio': 10, 'mask_prob_audio': 0.8, 'mask_length_image': 5, 'mask_prob_image': 0.3, 'mask_selection': static, 'mask_other': 0.0, 'no_mask_overlap': False, 'mask_min_space': 1, 'mask_channel_length': 10, 'mask_channel_prob': 0.0, 'mask_channel_selection': static, 'mask_channel_other': 0.0, 'no_mask_channel_overlap': False, 'mask_channel_min_space': 1, 'conv_pos': 128, 'conv_pos_groups': 16, 'latent_temp': [2.0, 0.5, 0.999995], 'skip_masked': False, 'skip_nomask': False, 'resnet_relu_type': 'prelu', 'resnet_weights': None, 'sim_type': 'cosine', 'sub_encoder_layers': 0, 'audio_feat_dim': 104, 'modality_dropout': 0.5, 'audio_dropout': 0.5, 'modality_fuse': 'concat', 'selection_type': 'same_seq', 'masking_type': 'input', 'decoder_embed_dim': 768, 'decoder_ffn_embed_dim': 3072, 'decoder_layers': 6, 'decoder_layerdrop': 0.0, 'decoder_attention_heads': 4, 'decoder_learned_pos': False, 'decoder_normalize_before': False, 'no_token_positional_embeddings': False, 'decoder_dropout': 0.1, 'decoder_attention_dropout': 0.1, 'decoder_activation_dropout': 0.0, 'max_target_positions': 2048, 'share_decoder_input_output_embed': False, 'no_scale_embedding': True}\n",
            "âœ… Loaded AV-HuBERT checkpoint\n",
            "Torch: 1.13.1+cu117 | CUDA: False\n",
            "Model: AVHubertModel | Task: AVHubertPretrainingTask\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Setup Directory Layout"
      ],
      "metadata": {
        "id": "Kxo6fX6crBIN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%bash\n",
        "# Make a tidy layout on your Google Drive\n",
        "mkdir -p \"/content/drive/MyDrive/AVSR_Colab\"/{data,manifests,experiments,models,logs}\n",
        "\n",
        "# Inside 'experiments', make a folder for your first run\n",
        "mkdir -p \"/content/drive/MyDrive/AVSR_Colab/experiments/baseline\"\n"
      ],
      "metadata": {
        "id": "PjGZ1NWDrG0u"
      },
      "execution_count": 36,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Point these at your Drive layout so later cells can reuse them\n",
        "BASE = \"/content/drive/MyDrive/AVSR_Colab\"\n",
        "DATA_DIR = f\"{BASE}/data\"              # where raw/augmented media will live\n",
        "MANIFEST_DIR = f\"{BASE}/manifests\"     # where TSVs (indexes) will live\n",
        "EXP_DIR = f\"{BASE}/experiments\"        # one subfolder per experiment\n",
        "MODELS_DIR = f\"{BASE}/models\"          # checkpoints you already have (and new ones)\n",
        "LOGS_DIR = f\"{BASE}/logs\"              # training logs / tensorboard etc.\n",
        "\n",
        "print(\"BASE       :\", BASE)\n",
        "print(\"DATA_DIR   :\", DATA_DIR)\n",
        "print(\"MANIFESTS  :\", MANIFEST_DIR)\n",
        "print(\"EXPERIMENTS:\", EXP_DIR)\n",
        "print(\"MODELS_DIR :\", MODELS_DIR)\n",
        "print(\"LOGS_DIR   :\", LOGS_DIR)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ndEjOmgSrPav",
        "outputId": "d05c63ba-85d6-4ae2-dc3e-603bfa80b411"
      },
      "execution_count": 45,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "BASE       : /content/drive/MyDrive/AVSR_Colab\n",
            "DATA_DIR   : /content/drive/MyDrive/AVSR_Colab/data\n",
            "MANIFESTS  : /content/drive/MyDrive/AVSR_Colab/manifests\n",
            "EXPERIMENTS: /content/drive/MyDrive/AVSR_Colab/experiments\n",
            "MODELS_DIR : /content/drive/MyDrive/AVSR_Colab/models\n",
            "LOGS_DIR   : /content/drive/MyDrive/AVSR_Colab/logs\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%%bash\n",
        "eval \"$(conda shell.bash hook)\"\n",
        "conda activate av_hubert\n",
        "\n",
        "python - << 'PY'\n",
        "import torch, importlib\n",
        "print(\"Torch:\", torch.__version__, \"| CUDA available:\", torch.cuda.is_available())\n",
        "try:\n",
        "    import fairseq\n",
        "    print(\"fairseq OK:\", getattr(fairseq, \"__file__\", None))\n",
        "except Exception as e:\n",
        "    print(\"fairseq import issue:\", e)\n",
        "\n",
        "# config stack the repo expects\n",
        "try:\n",
        "    from omegaconf import II, MISSING\n",
        "    import hydra\n",
        "    print(\"OmegaConf II/MISSING present âœ“ | Hydra:\", hydra.__version__)\n",
        "except Exception as e:\n",
        "    print(\"Config stack issue:\", e)\n",
        "PY\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "X0VmBPuRr74O",
        "outputId": "47c04467-25e1-4efa-b91e-df3bb14de3c5"
      },
      "execution_count": 38,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Torch: 1.13.1+cu117 | CUDA available: False\n",
            "fairseq OK: /content/av_hubert/fairseq/fairseq/__init__.py\n",
            "OmegaConf II/MISSING present âœ“ | Hydra: 1.1.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%%bash\n",
        "BASE=\"/content/drive/MyDrive/AVSR_Colab\"\n",
        "mkdir -p \"$BASE/experiments/baseline\"/{checkpoints,logs,manifests,configs} #create sub directories for baseline test\n",
        "echo \"Created scaffolds under $BASE/experiments/*\""
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "c_zuiXTisZii",
        "outputId": "7326f4cb-c2a6-452e-b82f-9b204c5f5e9f"
      },
      "execution_count": 48,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Created scaffolds under /content/drive/MyDrive/AVSR_Colab/experiments/*\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#freeze enironment (do this for each experiment so it can be replicated)\n",
        "%%bash\n",
        "eval \"$(conda shell.bash hook)\"\n",
        "conda activate av_hubert\n",
        "\n",
        "BASE=\"/content/drive/MyDrive/AVSR_Colab\"\n",
        "pip freeze > \"$BASE/experiments/baseline/configs/pip_freeze.txt\"\n",
        "python - << 'PY' > \"$BASE/experiments/baseline/configs/env_info.txt\"\n",
        "import torch, sys, platform\n",
        "print(\"python:\", sys.version)\n",
        "print(\"platform:\", platform.platform())\n",
        "print(\"torch:\", torch.__version__, \"cuda_available:\", torch.cuda.is_available())\n",
        "PY\n",
        "echo \"Saved env snapshot.\"\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mq-hfMwIscPA",
        "outputId": "461ff97d-5a92-48f6-852c-78e781a7586d"
      },
      "execution_count": 49,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Saved env snapshot.\n"
          ]
        }
      ]
    }
  ]
}